{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "Naive Bayes models the conditional probability of classes $C_k$, given an instance represented by a feature vector $x=(x_1, \\dots, x_n)$, as \n",
    "\\begin{align}\n",
    "p(C_k \\mid x) = \\frac{p(x \\mid C_k) p(C_k)}{p(x)}.\n",
    "\\end{align}\n",
    "The most important feature of the Naive Bayes model is that it assumes that all features are mutually independent conditional on the category $C_k$, e.g., \n",
    "\\begin{align}\n",
    "p(x_i \\mid x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n, C_k) = p(x_i \\mid C_k).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "The Naive Bayes classifier is based on the MAP (maximum a posteriori) estimate of the conditional probability $p(C_k, x)$, i.e., given a feature vector $x$, we predict it being of the class\n",
    "\\begin{align}\n",
    "\\hat{y} = \\text{argmax}_{k \\in [K]} p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k),\n",
    "\\end{align}\n",
    "or equivalently (for computational reasons)\n",
    "\\begin{align}\n",
    "\\hat{y} = \\text{argmax}_{k \\in [K]} \\left[ \\log \\left( p(C_k) \\right) + \\sum_{i=1}^n \\log \\left( p(x_i \\mid C_k \\right) \\right].\n",
    "\\end{align}\n",
    "For simplicity we assume here that $p(C_k) = c_k$ is constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling  the conditional probabilities\n",
    "One can choose any model for the conditional probabilities $p(x_i \\mid C_k)$, e.g., Gaussian, Bernoulli, Multinomial, etc. Note that the Naive Bayes classifier can easily handle mixtures of categorical and real-valued features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, class_prior, features, n_classes):\n",
    "        self.class_prior = class_prior\n",
    "        self.features = features\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        self.weights = []\n",
    "        \n",
    "        for i, feature in enumerate(self.features):\n",
    "            if feature == 'gaussian':\n",
    "                self.weights.append((self._random_normal(), self._random_normal()))\n",
    "            \n",
    "            elif feature == 'bernoulli':\n",
    "                self.weights.append((self._random_uniform()))\n",
    "     \n",
    "    def _random_normal(self, loc_in=0.0, scale_in=1.0, size_in=None):\n",
    "        if size_in is None:\n",
    "            size_in = self.n_classes\n",
    "        \n",
    "        return np.random.normal(loc=loc_in, scale=scale_in, size=size_in)\n",
    "    \n",
    "    def _random_uniform(self, low_in=0.0, high_in=1.0, size_in=None):\n",
    "        if size_in is None:\n",
    "            size_in = self.n_classes\n",
    "        \n",
    "        return np.random.uniform(low=low_in, high=high_in, size=size_in)\n",
    "        \n",
    "    def _conditional_log_probability(self, feature, class_index, feature_index):\n",
    "        if self.features[feature_index] == 'gaussian':\n",
    "            return - (1/2)*np.log(2*np.pi*self.weights[feature_index][1][class_index]**2) \\\n",
    "                   - (feature-self.weights[feature_index][0][class_index])**2 \\\n",
    "                   / (2*self.weights[feature_index][1][class_index]**2)\n",
    "        \n",
    "        elif self.features[feature_index] == 'bernoulli':\n",
    "            if feature == 0:\n",
    "                return np.log(1-self.weights[feature_index][class_index])\n",
    "            else:\n",
    "                return np.log(self.weights[feature_index][class_index])\n",
    "    \n",
    "    def log_likelihood(self, X, Y):\n",
    "        log_likelihood = 0.0\n",
    "        \n",
    "        for x, y in zip(X, Y):\n",
    "            sum_of_logs = 0.0\n",
    "            for j, _ in enumerate(self.features):\n",
    "                sum_of_logs += np.log(self.class_prior[y]) + self._conditional_log_probability(x[j], y, j)\n",
    "                \n",
    "            log_likelihood += sum_of_logs\n",
    "            \n",
    "        return log_likelihood\n",
    "    \n",
    "    ### assumes all features are gaussian\n",
    "    def gaussian_maximum_likelihood_fit(self, X, Y):\n",
    "        means = np.zeros((len(self.features), self.n_classes))\n",
    "        counts = np.zeros((self.n_classes,))\n",
    "        \n",
    "        for x, y in zip(X, Y):\n",
    "            for j, _ in enumerate(self.features):\n",
    "                means[j, y] += x[j]\n",
    "            counts[y] += 1\n",
    "                \n",
    "        print(counts)\n",
    "        for i in range(self.n_classes):\n",
    "             means[:, i] /= counts[i]\n",
    "                \n",
    "        variances = np.zeros((len(self.features), self.n_classes))\n",
    "        \n",
    "        for x, y in zip(X, Y):\n",
    "            for j, _ in enumerate(self.features):\n",
    "                variances[j, y] += (x[i] - means[i, y])**2 / counts[y]\n",
    "                \n",
    "        for i in range(self.n_classes):\n",
    "            for j, _ in enumerate(self.features):\n",
    "                self.weights[j][0][i] = means[j, i]\n",
    "                self.weights[j][1][i] = np.sqrt(variances[j, i])\n",
    "                \n",
    "    def predictions(self, X):\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            \n",
    "            class_predictions = []\n",
    "            for i in range(self.n_classes):\n",
    "                \n",
    "                class_predictions.append(self.log_likelihood([x], [i]))\n",
    "                \n",
    "            predictions.append(class_predictions)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood: -269720.4033337989\n",
      "[50. 50. 50.]\n",
      "Log-likelihood: -984.3869954533072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, Y = load_iris(return_X_y=True)\n",
    "X, Y = shuffle(X, Y)\n",
    "\n",
    "naive_bayes_iris = NaiveBayes([1/3.]*3, ['gaussian']*4, 3)\n",
    "\n",
    "print('Log-likelihood:', naive_bayes_iris.log_likelihood(X, Y))\n",
    "\n",
    "naive_bayes_iris.gaussian_maximum_likelihood_fit(X, Y)\n",
    "\n",
    "print('Log-likelihood:', naive_bayes_iris.log_likelihood(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes_iris.predictions(X[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9266666666666666"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y==np.argmax(np.array([predictions]), axis=2)) / 150"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
