{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "Naive Bayes models the conditional probability of classes $C_k$, given an instance represented by a feature vector $x=(x_1, \\dots, x_n)$, as \n",
    "\\begin{align}\n",
    "p(C_k \\mid x) = \\frac{p(x \\mid C_k) p(C_k)}{p(x)}.\n",
    "\\end{align}\n",
    "The most important feature of the Naive Bayes model is that it assumes that all features are mutually independent conditional on the category $C_k$, e.g., \n",
    "\\begin{align}\n",
    "p(x_i \\mid x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n, C_k) = p(x_i \\mid C_k)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "The Naive Bayes classifier is based on the MAP (maximum a posteriori) estimate, i.e., given a feature vector $x$, we predict it being of the class\n",
    "\\begin{align}\n",
    "\\hat{y} = \\text{argmax}_{k \\in [K]} p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k),\n",
    "\\end{align}\n",
    "or equivalently (for computational reasons)\n",
    "\\begin{align}\n",
    "\\hat{y} = \\text{argmax}_{k \\in [K]} \\left[ \\log \\left( p(C_k) \\right) + \\sum_{i=1}^n \\log \\left( p(x_i \\mid C_k \\right) \\right].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling  the conditional probabilities\n",
    "One can choose any model for the conditional probabilities $p(x_i \\mid C_k)$, e.g., Gaussian, Bernoulli, Multinomial, etc. Note that the Naive Bayes classifier can easily handle mixtures of categorical and real-valued features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_bayes:\n",
    "    def __init__(self, class_prior, features, n_classes):\n",
    "        self.class_prior = class_prior\n",
    "        self.features = features\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        _init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        self.weights = []\n",
    "        \n",
    "        for i, feature in enumerate(self.features):\n",
    "            if feature == 'gaussian':\n",
    "                self.weights.append((self._random_normal(), self._random_normal()))\n",
    "            \n",
    "            elif feature == 'bernoulli':\n",
    "                self.weights.append((self._random_uniform()))\n",
    "     \n",
    "    @staticmethod\n",
    "    def _random_normal(loc_in=0.0, scale_in=1.0, size_in=None):\n",
    "        if size_in is None:\n",
    "            size_in = self.n_classes\n",
    "        \n",
    "        return np.random.normal(loc=loc_in, scale=scale_in, size=size_in)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _random_uniform(low_in=0.0, high_in=1.0, size_in=None):\n",
    "        if size_in is None:\n",
    "            size_in = self.n_classes\n",
    "        \n",
    "        return np.random.uniform(low=low_in, high=high_in, size=size_in)\n",
    "        \n",
    "    def _log_probability(self, feature, class_index, feature_index):\n",
    "        if self.features[feature_index] == 'gaussian':\n",
    "            return - (1/2)*np.log(2*np.pi*self.weights[feature_index][1]**2) \\\n",
    "                   - (feature-self.weights[feature_index][0])**2/(2*self.weights[feature_index][1]**2)\n",
    "        \n",
    "        elif self.features[feature_index] == 'bernoulli':\n",
    "            if feature == 0:\n",
    "                return np.log(1-self.weights[feature_index])\n",
    "            else:\n",
    "                return np.log(self.weights[feature_index])\n",
    "        \n",
    "    def prediction(self, X):\n",
    "        for x in X:\n",
    "            predictions = []\n",
    "            for i in range(self.n_classes):\n",
    "                sum_of_logs = 0.0\n",
    "                for j, _ in enumerate(self.features):\n",
    "                    sum_of_logs += self._log_probability(x[j], i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
