{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful resources:\n",
    "1. https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vae-variational-autoencoder\n",
    "2. https://arxiv.org/pdf/1606.05908.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "Variational autoencoder are based on the principles of variational inference and graphical models. We want to generate data $x \\in \\mathcal{X}$; to do so we first split the joint probability of observable and latent variables into prior and likelihood\n",
    "\\begin{align}\n",
    "    p(x,z) = p(x \\mid z) p(z).\n",
    "\\end{align}\n",
    "If we have many latent variables, marginalization of the joint probability distribution is intractable. \n",
    "#### Model assumption and loss function\n",
    "In order to make the marginalization tractable, we model the posterior probability as \n",
    "\\begin{align}\n",
    "    q_{\\phi}(z \\mid x) \\approx p(z \\mid x).\n",
    "\\end{align}\n",
    "Obviously, we want the model to be very close to $p(z \\mid x)$; a measure of closeness is given by the Kullback--Leibler divergence \n",
    "\\begin{align}\n",
    "    D_{\\text{KL}}(q_\\phi || p) &= \\int q_\\phi \\log \\frac{q_\\phi}{p} \\, dx \\\\\n",
    "                                 &= \\int q_\\phi \\log \\frac{q_\\phi p(x)}{p(x,z)} \\, dx \\\\\n",
    "                                 &= \\log p(x) + \\int q_\\phi \\log \\frac{q_\\phi}{p(x \\mid z) p(z)} \\, dx \\\\   \n",
    "                                 &= \\log p(x) + D_{\\text{KL}}(q_\\phi || p(z)) - \\mathbb{E}_{q_\\phi}(p(x \\mid z)).\n",
    "\\end{align}\n",
    "Since we want to minimize, both, the negative log likelihood $- \\log p(x)$ as well as the difference between $q_\\phi$ and $p(z \\mid x)$, we the loss of our model to\n",
    "\\begin{align}\n",
    "    L_{\\text{VAE}} &= - \\log p(x) + D_{\\text{KL}}(q_\\phi || p) \\\\\n",
    "                   &= \\underbrace{D_{\\text{KL}}(q_\\phi || p(z)) - \\mathbb{E}_{q_\\phi}(p(x \\mid z))}_{\\text{Evidence lower bound (ELBO)}}. \\quad \\text{(using equation above)}\n",
    "\\end{align}\n",
    "#### Computing the KL divergence\n",
    "If we let both $p(z)$ and $q_\\phi$ be normally distributed, we can compute D_{\\text{KL}}(q_\\phi || p(z)) exactly. Let $q_\\phi = \\mathcal(\\mu_q, \\Sigma_q)$ and $p(z)=\\mathcal(\\mu_p, \\Sigma_p)$, then \n",
    "\\begin{align}\n",
    "    D_{\\text{KL}}(q_\\phi || p(z)) = \\frac{1}{2} \\left[ log \\frac{|\\Sigma_p|}{|\\Sigma_q|} - d + \\text{tr} \\left(\\Sigma_p^{-1} \\Sigma_q\\right) + (\\mu_p - \\mu_q)^T \\Sigma_p^{-1} (\\mu_p - \\mu_q) \\right].\n",
    "\\end{align}\n",
    "Generally, we choose $\\mu_p = 0$ and $\\Sigma_p = Id$; the KL divergence then simplifies to\n",
    "\\begin{align}\n",
    "\\frac{1}{2} \\left[ - log |\\Sigma_q| - d + \\text{tr} \\left(\\Sigma_q\\right) + \\mu_p^T \\mu_p \\right].\n",
    "\\end{align}\n",
    "#### Computing the expected value and the reparameterization trick\n",
    "Computing the expected value $\\mathbb{E}_{q_\\phi}(p(x \\mid z))$ involves sampling from a probability distribution. Backpropagation does not work here due to the stochastic nature of sampling, however, the reparameterization trick finds a way to make it all work out in the end. Instead of sampling $z$ directly from $z \\sim q_\\phi = \\mathcal{N}(z; \\mu_q, \\Sigma_q)$, we sample $\\epsilon \\sim \\mathcal{N}(0, Id)$ and compute $z=\\mu_q + \\Sigma_q \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    def __init__(self,\n",
    "                 encoder_network,\n",
    "                 decoder_network,\n",
    "                 ):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
