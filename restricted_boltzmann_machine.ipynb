{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful resources\n",
    "1. https://christian-igel.github.io/paper/TRBMAI.pdf\n",
    "2. https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probablisitc model\n",
    "A restricted Boltzmann machine is a special form of a Boltzmann machine. The nodes build a bipartite graph with one set being observable and the other one being hidden.\n",
    "#### Model assumption\n",
    "The probability distribution is modeled as \n",
    "\\begin{align}\n",
    "p(v,h) = \\frac{e^{-E(v,h)}}{Z},\n",
    "\\end{align}\n",
    "where $E$ is the energy function \n",
    "\\begin{align}\n",
    "E(v,h) = - \\sum_{i,j} w_{ij} v_i h_j - \\sum_i a_i v_i - \\sum_j b_j h_j,\n",
    "\\end{align}\n",
    "where v, h are the observable and hidden units, respectively; a, b, and w are weights associated with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a restricted Boltzmann machine\n",
    "In order to perform maximum likelihood training, we need the marginal distribution over the obvservable units\n",
    "\\begin{align}\n",
    "    p(v) = \\sum_h p(v,h).\n",
    "\\end{align}\n",
    "The derivative of the log-likelhood (for one data point) with respect to an arbitraty weight $\\theta$ is \n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial \\theta} &= \\frac{\\partial}{\\partial \\theta} \\left[ - \\log Z + \\log \\left( \\sum_h  e^{-E(v,h)} \\right) \\right] \\\\\n",
    "    &= - \\frac{1}{ \\sum_h  e^{-E(v,h)}}  \\sum_h  e^{-E(v,h)} \\frac{\\partial E(v,h)}{\\partial \\theta} + \\frac{1}{ \\sum_{v,h}  e^{-E(v,h)}}  \\sum_{v,h}  e^{-E(v,h)} \\frac{\\partial E(v,h)}{\\partial \\theta} \\\\\n",
    "    &= - \\underbrace{\\sum_h p(h \\mid v) \\frac{\\partial E(v,h)}{\\partial \\theta}}_{\\text{positive phase}} + \\underbrace{\\sum_{v,h} p(v,h) \\frac{\\partial E(v,h)}{\\partial \\theta}}_{\\text{negative phase}}.\n",
    "\\end{align}\n",
    "#### Conditional distributions\n",
    "Due to the assumption of the nodes being in a bipartite graph, we can easily caluclate the conditional distributions as\n",
    "\\begin{align}\n",
    "    p(h_j=1 \\mid v) = \\sigma \\left( \\sum_{i=1}^{n_{\\text{visible}}} w_{ij} v_i + b_j \\right),\n",
    "\\end{align}\n",
    "and \n",
    "\\begin{align}\n",
    "p(v_i=1 \\mid h) = \\sigma \\left( \\sum_{j=1}^{n_{\\text{hidden}}} w_{ij} h_j + a_i \\right).\n",
    "\\end{align}\n",
    "#### Factorization trick\n",
    "The positive phase can be cheaply calculated when using the factorization trick, e.g., for $\\theta = w_{ij}$ we get\n",
    "\\begin{align}\n",
    "    - \\sum_h p(h \\mid v) \\frac{\\partial E(v,h)}{\\partial w_{ij}} &= \\sum_h \\prod_{k=1} p(h_k \\mid v) v_i h_j \\\\\n",
    "    &= \\sum_{h_j} \\sum_{h_{-j}} p(h_j \\mid v) p(h_{-j} \\mid v) h_j v_i \\\\\n",
    "    &= \\sum_{h_j} p(h_j \\mid v) h_j v_i \\underbrace{\\left( \\sum_{h_{-j}} p(h_{-j} \\mid v) \\right)}_{=1} \\\\\n",
    "    &= \\sum_{h_j} p(h_j \\mid v) h_j v_i = p(h_j = 1 \\mid v) v_i.\n",
    "\\end{align}\n",
    "#### Learning algorithm \n",
    "Using the factorizatin trick, the derivatives of the log-likelihood (for onde data point) with respect to the connection weights $w_{ij}$, visible biases $a_i$ and hidden biases $b_j$ can be computed as \n",
    "\\begin{align}\n",
    "     \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial w_{ij}} &= p(h_j = 1 \\mid v) v_i - \\sum_v p(h_j = 1 \\mid v) v_i, \\\\\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial a_i} &= v_i - \\sum_v p(v) v_i, \\\\\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial b_i} &= p(h_j=1 \\mid v) - \\sum_v p(h_j = 1 \\mid v),\n",
    "\\end{align}\n",
    "respectively. For a large number of visible units, the sum is intractable as the summands increase exponentially, hence we need to approximate all gradients.\n",
    "#### Contrastive divergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
