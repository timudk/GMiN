{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful resources\n",
    "1. https://christian-igel.github.io/paper/TRBMAI.pdf\n",
    "2. https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probablisitc model\n",
    "A restricted Boltzmann machine is a special form of a Boltzmann machine. The nodes build a bipartite graph with one set being observable and the other one being hidden.\n",
    "#### Model assumption\n",
    "The probability distribution is modeled as \n",
    "\\begin{align}\n",
    "p(v,h) = \\frac{e^{-E(v,h)}}{Z},\n",
    "\\end{align}\n",
    "where $E$ is the energy function \n",
    "\\begin{align}\n",
    "E(v,h) = - \\sum_{i,j} w_{ij} v_i h_j - \\sum_i a_i v_i - \\sum_j b_j h_j,\n",
    "\\end{align}\n",
    "where v, h are the observable and hidden units, respectively; a, b, and w are weights associated with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a restricted Boltzmann machine\n",
    "In order to perform maximum likelihood training, we need the marginal distribution over the obvservable units\n",
    "\\begin{align}\n",
    "    p(v) = \\sum_h p(v,h).\n",
    "\\end{align}\n",
    "The derivative of the log-likelhood (for one data point) with respect to an arbitraty weight $\\theta$ is \n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial \\theta} &= \\frac{\\partial}{\\partial \\theta} \\left[ - \\log Z + \\log \\left( \\sum_h  e^{-E(v,h)} \\right) \\right] \\\\\n",
    "    &= - \\frac{1}{ \\sum_h  e^{-E(v,h)}}  \\sum_h  e^{-E(v,h)} \\frac{\\partial E(v,h)}{\\partial \\theta} + \\frac{1}{ \\sum_{v,h}  e^{-E(v,h)}}  \\sum_{v,h}  e^{-E(v,h)} \\frac{\\partial E(v,h)}{\\partial \\theta} \\\\\n",
    "    &= - \\underbrace{\\sum_h p(h \\mid v) \\frac{\\partial E(v,h)}{\\partial \\theta}}_{\\text{positive phase}} + \\underbrace{\\sum_{v,h} p(v,h) \\frac{\\partial E(v,h)}{\\partial \\theta}}_{\\text{negative phase}}.\n",
    "\\end{align}\n",
    "#### Conditional distributions\n",
    "Due to the assumption of the nodes being in a bipartite graph, we can easily caluclate the conditional distributions as\n",
    "\\begin{align}\n",
    "    p(h_j=1 \\mid v) = \\sigma \\left( \\sum_{i=1}^{n_{\\text{visible}}} w_{ij} v_i + b_j \\right),\n",
    "\\end{align}\n",
    "and \n",
    "\\begin{align}\n",
    "p(v_i=1 \\mid h) = \\sigma \\left( \\sum_{j=1}^{n_{\\text{hidden}}} w_{ij} h_j + a_i \\right).\n",
    "\\end{align}\n",
    "#### Factorization trick\n",
    "The positive phase can be cheaply calculated when using the factorization trick, e.g., for $\\theta = w_{ij}$ we get\n",
    "\\begin{align}\n",
    "    - \\sum_h p(h \\mid v) \\frac{\\partial E(v,h)}{\\partial w_{ij}} &= \\sum_h \\prod_{k=1} p(h_k \\mid v) v_i h_j \\\\\n",
    "    &= \\sum_{h_j} \\sum_{h_{-j}} p(h_j \\mid v) p(h_{-j} \\mid v) h_j v_i \\\\\n",
    "    &= \\sum_{h_j} p(h_j \\mid v) h_j v_i \\underbrace{\\left( \\sum_{h_{-j}} p(h_{-j} \\mid v) \\right)}_{=1} \\\\\n",
    "    &= \\sum_{h_j} p(h_j \\mid v) h_j v_i = p(h_j = 1 \\mid v) v_i.\n",
    "\\end{align}\n",
    "#### Learning algorithm \n",
    "Using the factorizatin trick, the derivatives of the log-likelihood (for onde data point) with respect to the connection weights $w_{ij}$, visible biases $a_i$ and hidden biases $b_j$ can be computed as \n",
    "\\begin{align}\n",
    "     \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial w_{ij}} &= p(h_j = 1 \\mid v) v_i - \\sum_v p(v) p(h_j = 1 \\mid v) v_i, \\\\\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial a_i} &= v_i - \\sum_v p(v) v_i, \\\\\n",
    "    \\frac{\\partial \\log \\mathcal{L}(\\theta \\mid v)}{\\partial b_i} &= p(h_j=1 \\mid v) - \\sum_v p(v) p(h_j = 1 \\mid v),\n",
    "\\end{align}\n",
    "respectively. For a large number of visible units, the sum is intractable as the summands increase exponentially, hence we need to approximate all gradients.\n",
    "#### Contrastive divergence\n",
    "We could approximate the second terms of the three equations above using block Gibbs sampling, however, this would require to run the Markov chain for a long time. It has been shown that estimates obtained after the chain for just a few steps sufficies for model training. This idea has led to the $k$-step constrastive divergence algorithm. We basically set $v^{(0)} = v$, where $v$ is a training example and sample $h^{(0)}$ from $p\\left(h \\mid v^{(0)} \\right)$. In the next step, $v^{(1)}$ is then sampled from $p \\left( v \\mid h^{(0)} \\right)$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestrictedBoltzmannMachine:\n",
    "    def __init__(self, init_weights, init_visible_biases, init_hidden_biases):\n",
    "        self.weights = init_weights\n",
    "        self.visible_biases = init_visible_biases\n",
    "        self.hidden_biases = init_hidden_biases\n",
    "        \n",
    "    @staticmethod\n",
    "    def _logistic_function(x):\n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "        \n",
    "    def _energy_function(self, v, h):\n",
    "        energy = 0\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                energy += - self.weights[i, j] * v[i] * h[j]\n",
    "        \n",
    "        for i, visible_bias in enumerate(self.visible_biases):\n",
    "            energy += - visible_bias * v[i]\n",
    "            \n",
    "        for j, hidden_bias in enumerate(self.hidden_biases):\n",
    "            energy += - hidden_bias * h[j]\n",
    "                \n",
    "        return energy\n",
    "    \n",
    "    def _conditional_h_given_v(self, v):\n",
    "        return self._logistic_function((self.weights.T @ v) + self.hidden_biases)\n",
    "    \n",
    "    def _conditional_v_given_h(self, h):\n",
    "        return self._logistic_function((self.weights @ h) + self.visible_biases)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _probability_to_logit(x):\n",
    "        for i in range(x.shape[0]):\n",
    "            u = np.random.uniform(0, 1)\n",
    "            if x[i] > u:\n",
    "                x[i] = 1.0\n",
    "            else:\n",
    "                x[i] = 0.0\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def _sampling_k_steps(self, v, k):\n",
    "        for _ in range(k):\n",
    "            h = self._probability_to_logit(self._conditional_h_given_v(v))\n",
    "            v = self._probability_to_logit(self._conditional_v_given_h(h))\n",
    "            \n",
    "        return v\n",
    "    \n",
    "    def block_gibbs_sampling(self, n_samples, n_burn_in=1000):\n",
    "        v = np.zeros((self.visible_biases.shape[0], 1))\n",
    "        \n",
    "        samples = []\n",
    "        for i in range(n_burn_in + n_samples):\n",
    "            v = self._sampling_k_steps(v, 1)\n",
    "\n",
    "            if i >= n_burn_in:\n",
    "                samples.append(v)\n",
    "                \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_step_contrastive_divergence(data, model, learning_rate=1.0, k=1):\n",
    "    for v in data:\n",
    "        v = v.reshape((model.visible_biases.shape[0], 1))\n",
    "        \n",
    "        sampled_v = model._sampling_k_steps(v, k)\n",
    "        \n",
    "        model.weights += learning_rate * (v @ model._conditional_h_given_v(v).T -\n",
    "                                          sampled_v @ model._conditional_h_given_v(sampled_v).T)\n",
    "        \n",
    "        model.visible_biases += learning_rate * (v - sampled_v)\n",
    "        \n",
    "        model.hidden_biases += learning_rate * (model._conditional_h_given_v(v) - \n",
    "                                                model._conditional_h_given_v(sampled_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "\n",
    "n_visible_units = 3\n",
    "n_hidden_units = 16\n",
    "n_points = 100\n",
    "n_steps = 100\n",
    "p = [.8, .15, .05]\n",
    "\n",
    "weights = np.random.normal(size=(n_visible_units, n_hidden_units))\n",
    "visible_biases = np.random.normal(size=(n_visible_units, 1))\n",
    "hidden_biases = np.random.normal(size=(n_hidden_units, 1))\n",
    "\n",
    "rbm = RestrictedBoltzmannMachine(weights, visible_biases, hidden_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### [[0, 0, 0], [0, 0, 1], [0, 1, 0], ..., [1, 1, 0], [1, 1, 1]]\n",
    "all_configs = list(product([0, 1], repeat=n_visible_units))\n",
    "\n",
    "data =  np.random.binomial(n=1, p=p, size=(n_points, n_visible_units))\n",
    "\n",
    "data_distribution = np.zeros((len(all_configs),))\n",
    "\n",
    "for x in data:\n",
    "    for j, config in enumerate(all_configs):\n",
    "        if np.sum(x == config) == n_visible_units:\n",
    "            data_distribution[j] += 1 / n_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "    k_step_contrastive_divergence(data, rbm, learning_rate=0.1, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sampling = 1000\n",
    "samples = rbm.block_gibbs_sampling(n_sampling, n_burn_in=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_distribution = np.zeros((len(all_configs),))\n",
    "\n",
    "for sample in samples:\n",
    "    for j, config in enumerate(all_configs):\n",
    "        if np.sum(sample.T == config) == n_visible_units:\n",
    "            sample_distribution[j] += 1 / n_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: (0, 0, 0) learned distribution: 0.018 data distribution: 0.130\n",
      "Configuration: (0, 0, 1) learned distribution: 0.001 data distribution: 0.000\n",
      "Configuration: (0, 1, 0) learned distribution: 0.013 data distribution: 0.040\n",
      "Configuration: (0, 1, 1) learned distribution: 0.009 data distribution: 0.010\n",
      "Configuration: (1, 0, 0) learned distribution: 0.769 data distribution: 0.690\n",
      "Configuration: (1, 0, 1) learned distribution: 0.008 data distribution: 0.010\n",
      "Configuration: (1, 1, 0) learned distribution: 0.171 data distribution: 0.120\n",
      "Configuration: (1, 1, 1) learned distribution: 0.011 data distribution: 0.000\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(all_configs):\n",
    "    print('Configuration:', all_configs[i], \n",
    "          'learned distribution: {0:.3f}'.format(sample_distribution[i]), \n",
    "          'data distribution: {0:.3f}'.format(data_distribution[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
