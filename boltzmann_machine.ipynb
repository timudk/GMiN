{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful resources\n",
    "1. http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec19.pdf\n",
    "2. http://willwolf.io/2018/10/20/thorough-introduction-to-boltzmann-machines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "A Boltzmann machine is a (fully observable) graphical model whose nodes are binary random variables, i.e., $x_i = \\{0, 1\\}$ for all $i=1, \\dots, n$. A bias $b_i$ determines how likely it is that $x_i=1$ and a weight $w_{ij}$ determines how likely it is that $x_i$ and $x_j$ take the same value. \n",
    "#### Model assumption\n",
    "A Boltzmann machine assumes that the joint probability distribution $p(x) = p(x_1, \\dots, x_n)$ can be modeled as \n",
    "\\begin{align}\n",
    "p(x) = \\frac{e^{H(x)}}{Z},\n",
    "\\end{align}\n",
    "where $H$ is the happiness function defined as \n",
    "\\begin{align}\n",
    "H(x) = \\sum_{\\substack{i=1 \\\\ j < i}}^n w_{ij} x_i x_j + \\sum_i b_i,\n",
    "\\end{align}\n",
    "and $Z$ is a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Boltzmann machine\n",
    "Given a dataset $\\left\\{X^{(j)}\\right\\}_{j \\in [m]}$, we can learn the weights and biases of the model by the principle of maximum likelihood. In contrast to the naive Bayes model, there is no closed-form solution for the parameters, however, we can update the parameters using gradient descent.\n",
    "\n",
    "#### Log-likelihood function\n",
    "The log-likelihood function $l$ is given as \n",
    "\\begin{align}\n",
    "l = \\left[ \\frac{1}{m} \\sum_{k=1}^m H(X^{(k)} \\right] - \\log Z,\n",
    "\\end{align}\n",
    "and it turns out that \n",
    "\\begin{align}\n",
    "    \\frac{\\partial l}{\\partial w_{rs}} &= \\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r X^{(k)}_s - \\sum_{x} p(x) x_r x_s, \\\\\n",
    "    \\frac{\\partial l}{\\partial b_{r}} &= \\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r- \\sum_{x} p(x) x_r,\n",
    "\\end{align}\n",
    "where $\\sum_x$ is the sum of all possible configurations of the Boltzmann machine.\n",
    "\n",
    "#### Learning algorithm\n",
    "We first initialize the weights and biases randomly and can then update them using gradient descent, i.e.,\n",
    "\\begin{align}\n",
    "    w^{n+1}_{rs} &\\gets w^{n}_{rs} + \\frac{\\partial l}{\\partial w^{n}_{rs}}, \\\\\n",
    "    b^{n+1}_{r} &\\gets b^{n}_{r} + \\frac{\\partial l}{\\partial b^{n}_{r}}.\n",
    "\\end{align}\n",
    "Note that the sum over all configurations of the Boltzmann machine has $2^n$ terms making exact gradient descent steps infeasible. Hence, we approximate $\\sum_{x} p(x) x_r x_s = \\mathbb{E}_{\\text{model}} [x_r x_s]$ and $\\sum_{x} p(x) x_r = \\mathbb{E}_{\\text{model}} [x_r]$ using a Markov chain Monte Carlo method. For big datasets, we could approximate $\\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r X^{(k)}_s = \\mathbb{E}_{\\text{data}} [x_r x_s]$ as well as $\\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r = \\mathbb{E}_{\\text{data}} [x_r]$ using mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "For sampling we use a Markov chain Monte Carlo method called Gibbs sampling. The reason for doing so is that Gibbs sampling is based on the conditional probability distributions\n",
    "\\begin{align}\n",
    "\\text{Pr}\\left(x_i = 1 \\mid x_{-i}\\right),\n",
    "\\end{align}\n",
    "which can be easily computed with our model assumption; note that $x_{-i} = \\{x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n\\}$. It turns out that \n",
    "\\begin{align}\n",
    "\\text{Pr}\\left(x_i = 1 \\mid x_{-i}\\right) = \\sigma \\left( \\sum_{j \\neq i} w_{ij} x_j + b_i \\right),\n",
    "\\end{align}\n",
    "where $\\sigma$ is the logistic function\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp ( -x )}.\n",
    "\\end{align}\n",
    "As usual with Markov chain Monte Carlo methods, Gibbs sampling generates a Markov chain of samples. Generally, samples from the beginining of the chain might not represent the true probability distribution well, and therefore we discard the first $n_{\\text{burn-in}}$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannMachine:\n",
    "    def __init__(self, init_weights, init_biases, configs):\n",
    "        self.weights = init_weights\n",
    "        self.biases = init_biases\n",
    "        self.configs = configs\n",
    "        \n",
    "    @staticmethod\n",
    "    def _logistic_function(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def _happiness_function(self, x):\n",
    "        happiness = 0\n",
    "        \n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(i):\n",
    "                happiness += self.weights[i, j] * x[i] * x[j]\n",
    "        \n",
    "        for i, _ in enumerate(self.biases):\n",
    "            happiness += self.biases[i] * x[i]\n",
    "                \n",
    "        return happiness\n",
    "    \n",
    "    def likelihood(self, x, compute_log_likelihood=True):\n",
    "        exponential_happiness = [np.exp(self._happiness_function(x_i)) for x_i in x]        \n",
    "        \n",
    "        Z = sum([np.exp(self._happiness_function(config)) for config in self.configs])\n",
    "        \n",
    "        if compute_log_likelihood == True:\n",
    "            return sum([np.log(exp_happ)  - np.log(Z) for exp_happ in exponential_happiness])\n",
    "        else: \n",
    "            return reduce(np.multiply, [exp_happ / Z for exp_happ in exponential_happiness])\n",
    "        \n",
    "    def gibbs_sampling(self, n_samples, n_burn_in=1000):\n",
    "        samples = []\n",
    "        samples.append([0 for _ in self.biases])\n",
    "        \n",
    "        def _gibbs_step(sample, i):\n",
    "            z = sum([self.weights[i, j] * sample[j] for j in range(len(sample)) if j != i]) + self.biases[i]\n",
    "            p = self._logistic_function(z)\n",
    "            return np.random.binomial(n=1, p=p)\n",
    "        \n",
    "        for _ in range(n_samples + n_burn_in):\n",
    "            sample = list(samples[-1])\n",
    "            for i, _ in enumerate(sample):\n",
    "                sample[i] = _gibbs_step(sample=sample, i=i)\n",
    "\n",
    "            samples.append(sample)\n",
    "\n",
    "        return np.array([sample for i, sample in enumerate(samples[n_burn_in:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two functions to update the weights of the Boltzmann machine. The first function computes $\\mathbb{E}_{\\text{model}} [x_r x_s]$ and $\\mathbb{E}_{\\text{model}} [x_r]$ exactly, whereas the second function approximates those quantities using Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, model, learning_rate):\n",
    "    model_distribution = [(np.array(config), model.likelihood(np.array([config]), compute_log_likelihood=False)) \\\n",
    "                           for config in model.configs]\n",
    "    ### weights\n",
    "    for i in range(model.weights.shape[0]):\n",
    "        for j in range(i):\n",
    "            w_positive = (data[:, i] * data[:, j]).mean()\n",
    "\n",
    "            w_negative = sum([config[i] * config[j] * likelihood for config, likelihood in model_distribution])\n",
    "\n",
    "            model.weights[i, j] += learning_rate * (w_positive - w_negative)\n",
    "    \n",
    "    ### biases\n",
    "    for i in range(model.biases.shape[0]):\n",
    "        b_positive = data[:, i].mean()\n",
    "        \n",
    "        b_negative = sum([config[i] * likelihood for config, likelihood in model_distribution])\n",
    "        \n",
    "        model.biases[i] += learning_rate * (b_positive - b_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_gradient_descent_using_gibbs_sampling(data, model, learning_rate, n_samples):\n",
    "    gibbs_samples = model.gibbs_sampling(n_samples)\n",
    "\n",
    "    ### weights\n",
    "    for i in range(model.weights.shape[0]):\n",
    "        for j in range(i):\n",
    "            w_positive = (data[:, i] * data[:, j]).mean()\n",
    "\n",
    "            w_negative = (gibbs_samples[:, i] * gibbs_samples[:, j]).mean()\n",
    "\n",
    "            model.weights[i, j] += learning_rate * (w_positive - w_negative)\n",
    "    \n",
    "    ### biases\n",
    "    for i in range(model.biases.shape[0]):\n",
    "        b_positive = data[:, i].mean()\n",
    "        \n",
    "        b_negative = (gibbs_samples[:, i]).mean()\n",
    "        \n",
    "        model.biases[i] += learning_rate * (b_positive - b_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_units = 3\n",
    "n_points = 100\n",
    "n_steps = 1000\n",
    "learning_rate = 0.1 \n",
    "p = [.8, .1, .5]\n",
    "\n",
    "### [[0, 0, 0], [0, 0, 1], [0, 1, 0], ..., [1, 1, 0], [1, 1, 1]]\n",
    "all_configs = list(product([0, 1], repeat=n_units))\n",
    "\n",
    "biases = np.random.randn(n_units)\n",
    "### we initialize weights as a n_units by n_units, but actually just use the elements below the main diagonal\n",
    "weights = np.random.randn(n_units, n_units)\n",
    "\n",
    "boltzmann_machine = BoltzmannMachine(weights, biases, all_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are producing some toy data in 3 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  np.random.binomial(n=1, p=p, size=(n_points, n_units))\n",
    "\n",
    "data_distribution = np.zeros((len(all_configs),))\n",
    "\n",
    "for x in data:\n",
    "    for j, config in enumerate(all_configs):\n",
    "        if np.sum(x == config) == n_units:\n",
    "            data_distribution[j] += 1 / n_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now training the Boltzmann machine using $n_{\\text{steps}}$ steps of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Log-likelihood: -288.0336365242908\n",
      "Epoch: 100 | Log-likelihood: -154.3247149791821\n",
      "Epoch: 200 | Log-likelihood: -152.98770974403578\n",
      "Epoch: 300 | Log-likelihood: -152.66784399515788\n",
      "Epoch: 400 | Log-likelihood: -152.4571187752501\n",
      "Epoch: 500 | Log-likelihood: -152.30205110294634\n",
      "Epoch: 600 | Log-likelihood: -152.1863146368242\n",
      "Epoch: 700 | Log-likelihood: -152.09938160789244\n",
      "Epoch: 800 | Log-likelihood: -152.03375796159676\n",
      "Epoch: 900 | Log-likelihood: -151.98401084176203\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_steps):\n",
    "    gradient_descent(data, boltzmann_machine, learning_rate)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        log_likelihood = boltzmann_machine.likelihood(data)\n",
    "        print(f'Epoch: {i:2} | Log-likelihood: {log_likelihood}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not print the probability of each configuration learned by the Boltzmann machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: (0, 0, 0) learned distribution: 0.089 data distribution: 0.100\n",
      "Configuration: (0, 0, 1) learned distribution: 0.044 data distribution: 0.040\n",
      "Configuration: (0, 1, 0) learned distribution: 0.025 data distribution: 0.020\n",
      "Configuration: (0, 1, 1) learned distribution: 0.011 data distribution: 0.010\n",
      "Configuration: (1, 0, 0) learned distribution: 0.353 data distribution: 0.350\n",
      "Configuration: (1, 0, 1) learned distribution: 0.381 data distribution: 0.380\n",
      "Configuration: (1, 1, 0) learned distribution: 0.050 data distribution: 0.050\n",
      "Configuration: (1, 1, 1) learned distribution: 0.047 data distribution: 0.050\n"
     ]
    }
   ],
   "source": [
    "model_distribution = [(np.array(config), \n",
    "                       boltzmann_machine.likelihood(np.array([config]), \n",
    "                       compute_log_likelihood=False)) \n",
    "                      for config in boltzmann_machine.configs]\n",
    "\n",
    "for i, _ in enumerate(boltzmann_machine.configs):\n",
    "    print('Configuration:', all_configs[i], \n",
    "          'learned distribution: {0:.3f}'.format(model_distribution[i][1]), \n",
    "          'data distribution: {0:.3f}'.format(data_distribution[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to produce samples using Gibbs sampling and compare the sample distribution to the learned distribution/data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "samples = boltzmann_machine.gibbs_sampling(n_samples=10000)\n",
    "\n",
    "sampling_distribution = np.zeros((len(all_configs),))\n",
    "\n",
    "for sample in samples:\n",
    "    for j, config in enumerate(all_configs):\n",
    "        if np.sum(sample == config) == n_units:\n",
    "            sampling_distribution[j] += 1 / n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: (0, 0, 0) sampling distribution: 0.083\n",
      "Configuration: (0, 0, 1) sampling distribution: 0.038\n",
      "Configuration: (0, 1, 0) sampling distribution: 0.027\n",
      "Configuration: (0, 1, 1) sampling distribution: 0.011\n",
      "Configuration: (1, 0, 0) sampling distribution: 0.344\n",
      "Configuration: (1, 0, 1) sampling distribution: 0.372\n",
      "Configuration: (1, 1, 0) sampling distribution: 0.065\n",
      "Configuration: (1, 1, 1) sampling distribution: 0.059\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(boltzmann_machine.configs):\n",
    "    print('Configuration:', all_configs[i], \n",
    "          'sampling distribution: {0:.3f}'.format(sampling_distribution[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train a model using the approximate gradient descent method (and the same data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 3\n",
    "n_points = 100\n",
    "n_steps = 1000\n",
    "learning_rate = 0.1 \n",
    "p = np.random.uniform(0, 1, size=(n_units,))\n",
    "\n",
    "all_configs = list(product([0, 1], repeat=n_units))\n",
    "\n",
    "biases = np.random.randn(n_units)\n",
    "weights = np.random.randn(n_units, n_units)\n",
    "\n",
    "boltzmann_machine = BoltzmannMachine(weights, biases, all_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Log-likelihood: -258.1925192118695\n",
      "Epoch: 100 | Log-likelihood: -153.3524918808026\n",
      "Epoch: 200 | Log-likelihood: -153.7383024413453\n",
      "Epoch: 300 | Log-likelihood: -153.6838978949054\n",
      "Epoch: 400 | Log-likelihood: -154.05059012518075\n",
      "Epoch: 500 | Log-likelihood: -153.9162049494429\n",
      "Epoch: 600 | Log-likelihood: -154.15105041690703\n",
      "Epoch: 700 | Log-likelihood: -154.0103952779528\n",
      "Epoch: 800 | Log-likelihood: -154.17221830910893\n",
      "Epoch: 900 | Log-likelihood: -154.22467600409564\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "\n",
    "for i in range(n_steps):\n",
    "    approximate_gradient_descent_using_gibbs_sampling(data, boltzmann_machine, learning_rate, n_samples)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        log_likelihood = boltzmann_machine.likelihood(data)\n",
    "        print(f'Epoch: {i:2} | Log-likelihood: {log_likelihood}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: (0, 0, 0) learned distribution: 0.055 data distribution: 0.100\n",
      "Configuration: (0, 0, 1) learned distribution: 0.025 data distribution: 0.040\n",
      "Configuration: (0, 1, 0) learned distribution: 0.013 data distribution: 0.020\n",
      "Configuration: (0, 1, 1) learned distribution: 0.005 data distribution: 0.010\n",
      "Configuration: (1, 0, 0) learned distribution: 0.380 data distribution: 0.350\n",
      "Configuration: (1, 0, 1) learned distribution: 0.429 data distribution: 0.380\n",
      "Configuration: (1, 1, 0) learned distribution: 0.046 data distribution: 0.050\n",
      "Configuration: (1, 1, 1) learned distribution: 0.047 data distribution: 0.050\n"
     ]
    }
   ],
   "source": [
    "model_distribution = [(np.array(config), \n",
    "                       boltzmann_machine.likelihood(np.array([config]), \n",
    "                       compute_log_likelihood=False)) \n",
    "                      for config in boltzmann_machine.configs]\n",
    "\n",
    "for i, _ in enumerate(boltzmann_machine.configs):\n",
    "    print('Configuration:', all_configs[i], \n",
    "          'learned distribution: {0:.3f}'.format(model_distribution[i][1]), \n",
    "          'data distribution: {0:.3f}'.format(data_distribution[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
