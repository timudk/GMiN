{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model\n",
    "A Boltzmann machine is a (fully observable) graphical model whose nodes are binary random variables, i.e., $x_i = \\{0, 1\\}$ for all $i=1, \\dots, n$. A bias $b_i$ determines how likely it is that $x_i=1$ and a weight $w_{ij}$ determines how likely it is that $x_i$ and $x_j$ take the same value. \n",
    "#### Model assumption\n",
    "A Boltzmann machine assumes that the joint probability distribution $p(x) = p(x_1, \\dots, x_n)$ can be modeled as \n",
    "\\begin{align}\n",
    "p(x) = \\frac{e^{H(x)}}{Z},\n",
    "\\end{align}\n",
    "where $H$ is the happiness function defined as \n",
    "\\begin{align}\n",
    "H(x) = \\sum_{i \\neq j} w_{ij} x_i x_j + \\sum_i b_i,\n",
    "\\end{align}\n",
    "and $Z$ is a normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning a Boltzmann machine\n",
    "Given a dataset $\\left\\{X^{(j)}\\right\\}_{j \\in [m]}$, we can learn the weights and biases of the model by the principle of maximum likelihood. In contrast to the naive Bayes model, there is no closed-form solution for the parameters, however, we can update the parameters using gradient descent.\n",
    "\n",
    "#### Log-likelihood function\n",
    "The log-likelihood function $l$ is given as \n",
    "\\begin{align}\n",
    "l = \\left[ \\frac{1}{m} \\sum_{k=1}^m H(X^{(k)} \\right] - \\log Z,\n",
    "\\end{align}\n",
    "and it turns out that \n",
    "\\begin{align}\n",
    "    \\frac{\\partial l}{\\partial w_{rs}} &= \\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r X^{(k)}_s - \\sum_{x} p(x) x_r x_s, \\\\\n",
    "    \\frac{\\partial l}{\\partial b_{r}} &= \\frac{1}{m} \\sum_{k=1}^m X^{(k)}_r- \\sum_{x} p(x) x_r,\n",
    "\\end{align}\n",
    "where $\\sum_x$ is the sum of all possible configurations of the Boltzmann machine.\n",
    "\n",
    "#### Learning algorithm\n",
    "We first initialize the weights and biases randomly and can then update them using gradient descent, i.e.,\n",
    "\\begin{align}\n",
    "    w^{n+1}_{rs} &\\gets w^{n}_{rs} + \\frac{\\partial l}{\\partial w^{n}_{rs}}, \\\\\n",
    "    b^{n+1}_{r} &\\gets b^{n}_{r} + \\frac{\\partial l}{\\partial b^{n}_{r}}.\n",
    "\\end{align}\n",
    "Note that the sum over all configurations of the Boltzmann machine has $2^n$ terms making exact gradient descent steps infeasible. Hence, we approximate $\\sum_{x} p(x) x_r x_s = \\mathbb{E}_{\\text{model}} [x_r x_s]$ and $\\sum_{x} p(x) x_r = \\mathbb{E}_{\\text{model}} [x_r]$ using a Markov chain Monte Carlo method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "For sampling we use a Markov chain Monte Carlo method called Gibbs sampling. The reason for doing so is that Gibbs sampling is based on the conditional probability distributions\n",
    "\\begin{align}\n",
    "\\text{Pr}\\left(x_i = 1 \\mid x_{-i}\\right),\n",
    "\\end{align}\n",
    "which can be easily computed with our model assumption; note that $x_{-i} = \\{x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n\\}$. It turns out that \n",
    "\\begin{align}\n",
    "\\text{Pr}\\left(x_i = 1 \\mid x_{-i}\\right) = \\sigma \\left( \\sum_{i \\neq j} w_{ij} x_j + b_i \\right),\n",
    "\\end{align}\n",
    "where $\\sigma$ is the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
